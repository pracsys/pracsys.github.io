@inproceedings{zhou2022ungraspable,
  title={Learning to Grasp the Ungraspable with Emergent Extrinsic Dexterity},
  author={Zhou, Wenxuan and Held, David},
  booktitle={Conference on Robot Learning (CoRL)},
  year={2022},
  tags="manipulation, learning",
  img="ungraspable.gif",
  projecturl="https://sites.google.com/view/grasp-ungraspable",
  paperurl="https://arxiv.org/abs/2211.01500",
  abstract="A simple gripper can solve more complex manipulation tasks if it can utilize the external environment such as pushing the object against the table or a vertical wall, known as \"Extrinsic Dexterity.\" Previous work in extrinsic dexterity usually has careful assumptions about contacts which impose restrictions on robot design, robot motions, and the variations of the physical parameters. In this work, we develop a system based on reinforcement learning (RL) to address these limitations. We study the task of “Occluded Grasping” which aims to grasp the object in configurations that are initially occluded; the robot needs to move the object into a configuration from which these grasps can be achieved. We present a system with model-free RL that successfully achieves this task using a simple gripper with extrinsic dexterity. The policy learns emergent behaviors of pushing the object against the wall to rotate and then grasp it without additional reward terms on extrinsic dexterity. We discuss important components of the system including the design of the RL problem, multi-grasp training and selection, and policy generalization with automatic curriculum. Most importantly, the policy trained in simulation is zero-shot transferred to a physical robot. It demonstrates dynamic and contact-rich motions with a simple gripper that generalizes across objects with various size, density, surface friction, and shape with a 78% success rate."
  }

@inproceedings{pan2022tax,
    title={TAX-Pose: Task-Specific Cross-Pose Estimation for Robot Manipulation},
    author={Pan, Chuer and Okorn, Brian and Zhang, Harry and Eisner, Ben and Held, David},
    booktitle={Conference on Robot Learning (CoRL)},
    year={2022},
    tags="manipulation, learning, imitation",
    img="taxpose-web.gif",
    projecturl="https://sites.google.com/view/tax-pose/home",
    paperurl="https://arxiv.org/pdf/2211.09325.pdf",
    abstract="How do we imbue robots with the ability to efficiently manipulate unseen objects and transfer relevant skills based on demonstrations? End-to-end learning methods often fail to generalize to novel objects or unseen configurations. Instead, we focus on the task-specific pose relationship between relevant parts of interacting objects. We conjecture that this relationship is a generalizable notion of a manipulation task that can transfer to new objects in the same category; examples include the relationship between the pose of a pan relative to an oven or the pose of a mug relative to a mug rack. We call this task-specific pose relationship “cross-pose” and provide a mathematical definition of this concept. We propose a vision-based system that learns to estimate the cross-pose between two objects for a given manipulation task using learned cross-object correspondences. The estimated cross-pose is then used to guide a downstream motion planner to manipulate the objects into the desired pose relationship (placing a pan into the oven or the mug onto the mug rack). We demonstrate our method’s capability to generalize to unseen objects, in some cases after training on only 10 demonstrations in the real world. Results show that our system achieves state-of-the-art performance in both simulated and real-world experiments across a number of tasks."
}

@inproceedings{huang2022medor,
    title={Mesh-based Dynamics Model with Occlusion Reasoning for Cloth Manipulation},
    author={Huang, Zixuan and Lin, Xingyu and Held,David},
    booktitle={Robotics: Science and Systems (RSS)},
    year={2022},
    tags="manipulation, Computer Vision",
    img="rss2022medor.gif",
    projecturl="https://sites.google.com/view/occlusion-reason/home",
    paperurl="https://arxiv.org/abs/2206.02881",
    posterurl="https://drive.google.com/file/d/1xdMVN7moNcdIeAPuPP0tpdvtjIfeFWdZ/view?usp=sharing",
    videourl="https://youtu.be/0s9PA6EgiqE",
    abstract="Self-occlusion is challenging for cloth manipulation, as it makes it difficult to estimate the full state of the cloth. Ideally, a robot trying to unfold a crumpled or folded cloth should be able to reason about the cloth's occluded regions. We leverage recent advances in pose estimation for cloth to build a system that uses explicit occlusion reasoning to unfold a crumpled cloth. Specifically, we first learn a model to reconstruct the mesh of the cloth. However, the model will likely have errors due to the complexities of the cloth configurations and due to ambiguities from occlusions. Our main insight is that we can further refine the predicted reconstruction by performing test-time finetuning with self-supervised losses. The obtained reconstructed mesh allows us to use a mesh-based dynamics model for planning while reasoning about occlusions. We evaluate our system both on cloth flattening as well as on cloth canonicalization, in which the objective is to manipulate the cloth into a canonical pose. Our experiments show that our method significantly outperforms prior methods that do not explicitly account for occlusions or perform test-time optimization."
}